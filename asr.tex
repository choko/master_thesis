\chapter{Theoretical Background}
This chapter describes theory needed for the CloudASR platform.
It starts with \textbf{Automatic Speech Recognition} section,
  which introduces concepts such as acoustic models, language models or speech decoding.
Then, sections \textbf{Open-Source ASR Tools} and \textbf{Public ASR services} present technologies related to CloudASR.
Finally, this chapter ends with \textbf{Obtaining Manual Transcriptions} section,
  which shows how manual transcriptions can be obtained with crowd--sourcing.

\section{Automatic Speech Recognition}
The task of the automatic speech recognition system is to "pick the most likely word sequence $\widehat{W}$
  given the observed acoustic evidence A" \cite{frederick1997statistical}.
Therefore, speech recognition can be described as a following formula:

\begin{equation}
  \label{eq:1}
  \widehat{W} = \arg \max_{W} P(W|A)
\end{equation}

By using Bayes' formula of probability theory,
  right-hand side of the Equation~\ref{eq:1} can be rewritten in a following way:

\begin{equation}
 \widehat{W} = \arg \max_{W} \frac{P(A|W) P(W)}{P(A)}
\end{equation}

Since the numerator P(A) is constant regarding the maximization,
  it can be omitted to get the final equation:

\begin{equation}
  \label{eq:asr}
  \widehat{W} = \arg \max_{W} P(A|W) P(W)
\end{equation}

Then, the probability $P(A|W)$ is called acoustic model and the probability $P(W)$ is called language model.
In the sections \textbf{Acoustic Models} and \textbf{Language Models} computation of these probabilities will be described in more detail.
After that, \textbf{Speech Decoding} section will describe how the Formula~\ref{eq:asr} is used to find the desired word sequence.


\subsection{Acoustic Models}
\BLIND
\BLIND

Baum--Welch \cite{welch2003hidden}
Viterbi training \cite{franzini1990connectionist}

\subsection{Language Models}
The task of the language model $P(W)$ in speech recognition is
  to determine how likely are the sequences of words $w_1,\dots,w_m$ that sound alike
  by assigning a probability to each sequence.
Using the Bayes' rule, $P(W)$ can be seen as:

\begin{equation}
  P(W) = P(w_1,\dots,w_m) = \prod\limits_{i=1}^{m} P(w_i|w_1,\dots,w_{i-1})
\end{equation}

Since the probability $P(w_i|w_1,\dots,w_{i-1})$ has just too many arguments
  and the probability does not necessarily depends on the entire history,
  the history is put into equivalence classes $\phi(w_1,\dots,w_{i-1})$.
This results into the following formula:

\begin{equation}
  P(w_1,\dots,w_m) \approx \prod\limits_{i=1}^{m} P(w_i|\phi(w_1,\dots,w_{i_1}))
\end{equation}

Traditionally, n-gram language models,
  which use the following history equivalence classes $\phi(w_1,\dots,w_{i-1}) = w_{i-(n-1)},\dots,w_{i-1}$,
  are used in speech recognition tasks.
Thus, the n-gram language model becomes:

\begin{equation}
  P(w_1,\dots,w_m) \approx \prod\limits_{i=1}^{m} P(w_i|w_{i-(n-1)},\dots,w_{i-1})
\end{equation}

Where the probabilities $P(w_i|w_{i-(n-1)},\dots,w_{w-1})$ are estimated from the relative frequencies of n-grams in the training data with the following formula:
\begin{equation}
  \label{eq:2}
  P(w_i|w_{i-(n-1)},\dots,w_{i-1}) = \frac{c(w_{i-(n-1)},\dots,w_i)}{c(w_{i-(n-1)},\dots,w_{i-1})}
\end{equation}


Since the numerator of the Equation~\ref{eq:2} can be zero due to data sparsity problem,
  several smoothing techniques such as
  Jelinek--Mercer \cite{jelinek1980interpolated},
  Good--Turing \cite{gale1995good}
  or Kneser--Ney \cite{kneser1995improved} are often used to estimate the higher n-gram relative frequencies from the lower n-gram frequencies.

Recently, artificial neural networks have been also successfully used to tackle the data sparsity problem \cite{bengio2003neural}.
For instance, the recurrent neural network based language models yielded state-of-the-art results in terms of WER in speech recognition \cite{mikolov2010recurrent}.


\subsection{Speech Decoding}
\BLIND
\BLIND


\section{Open-Source ASR Tools}
In the following section some of the open-source ASR tools will be described.
Namely, \textbf{HTK}, \textbf{Julius}, \textbf{Kaldi} and \textbf{RWTH}.


\textbf{HTK} \cite{young1997htk} is the first toolkit, that will be described.
HTK is a toolkit for building and manipulating hidden Markov models.
It consists of a set of library modules and tools that provide sophisticated facilities for speech analysis, HMM training, testing and results analysis.
Furthermore, it supports HMMs using both continuous density mixture Gaussians and discrete distributions and can be used to build complex HMM systems.

The second toolkit is \textbf{Julius} \cite{lee2001julius}, high-performance large vocabulary speech recognition decoder,
  that can perform almost real-time decoding with 60k words in the vocabulary.
It supports statistical n-gram language model and rule-based grammars,
  as well as Hidden Markov Model (HMM) as an acoustic model
Julius can be also used with models trained for HTK toolkit.

Next described toolkit is \textbf{Kaldi} \cite{povey2011kaldi} -- a free, open-source toolkit for speech recognition written in C++.
Its speech recognition system is based on finite-state transducers
  and it supports modelling of arbitrary phonetic-context sizes,
  acoustic modelling with subspace Gaussian mixture models (SGMM)
  as well as standard Gaussian mixture models and deep neural networks,
  together with all commonly used linear and affine transforms.

Furthermore, there is also a Python wrapper for Kaldi called PyKaldi \cite{platek2014free},
  which supports the online speech recognition.
CloudASR uses PyKaldi as a default speech recognition system.

The last toolkit that will be described is \textbf{RWTH} \cite{rybach2009rwth},
  which is a publicly available speech recognition toolkit developed at Aachen University.
It includes state of the art speech recognition technology for acoustic model training and decoding.
Besides, its notable components are speaker adaptation,
  speaker adaptive training,
  unsupervised training,
  a finite state automata library,
  and an efficient tree search decoder.


\section{Public ASR services}
In addition to these open sources ASR toolkits
  there are also several web services that provide an API for speech recognition.
Some of these services will be described in the following section.

\textbf{Google Speech API} supports speech recognition for 39 languages and their dialects.
Its batch API, illustrated in Figure~\ref{fig:google-api}, is very simple and can be used for transcription of the wave or flac files.
Additionally,
  Google Speech API supports the online speech recognition mode through JavaScript class SpeechRecognition in Google Chrome web browser.\footnote{\url{https://www.google.com/intl/en/chrome/demos/speech.html}}

\begin{figure}[h]
  \verbatiminput{snippets/google-api.bash}

  \caption{An example of Google Speech API batch speech recognition mode request for a transcription of a recording in British English.}
  \label{fig:google-api}
\end{figure}


\textbf{Nuance Dragon NaturallySpeaking}\footnote{\url{http://www.nuance.com/for-developers/dragon/index.htm}}
  is the second provider of the API for speech recognition.
It provides software development kits for Windows and mobile applications.
It also has a version that can be deployed on a server and used as a API for other applications.


The last API provider that will be mentioned here is \textbf{wit.ai}\footnote{\url{https://wit.ai/}}.
It supports 11 languages via an API similar to Google Speech API, see Figure~\ref{fig:wit-ai-request} for an example,
  and in addition to speech recognition it also supports intent classification of the submitted recordings,
  see Figure~\ref{fig:wit-ai-response} for an exemplar response from the wit.ai.

\begin{figure}[h]
  \verbatiminput{snippets/google-api.bash}

  \caption{An example of wit.ai API request for a transcription of a recording.}
  \label{fig:wit-ai-request}
\end{figure}

\begin{figure}[h]
  \verbatiminput{snippets/wit-ai-response.json}

  \caption{An exemplar response from wit.ai API with a recording of a sentence: "I'm looking for a bar."}
  \label{fig:wit-ai-response}
\end{figure}


\section{Obtaining Manual Transcriptions}
A large amount of transcribed recordings is needed in order to train a good ASR system.
But a manual transcription of the recordings by professional transcribers is expensive and time demanding,
  typically, professional transcribers need 6 hours to transcribe 1 hour of speech data \cite{williams2011crowd}.
Furthermore, it is difficult to find enough professional transcribers to transcribe the required amount of speech data in short time.

Recently, it was shown that crowd-sourcing can be used for cheap, fast and good enough manual transcription of speech data \cite{novotney2010cheap}.
With crowd-sourcing, speech data is splitted into small chunks that are then transcribed by several non-professional transcribers.
Their transcriptions are then used to select the best transcription for the recording, for example with ROVER algorithm \cite{marge2010using}, and used for training of new ASR systems.
Also, transcriptions from non-professional transcribers are only $6\%$ worse than professional transcriptions
  and they cost only $\frac{1}{30}$ of the cost of professional transcription.
Finally, services like \textbf{Amazon Mechanical Turk}\footnote{\url{https://www.mturk.com}}
  or \textbf{CrowdFlower}\footnote{\url{http://www.crowdflower.com/}} already support the speech transcription tasks.
